{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 -> Designed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset consists of\n",
    "\n",
    "60000 32x32 colour images in 10 classes\n",
    "\n",
    "with 6000 images per class\n",
    "\n",
    "There are 50000 training images and 10000 test images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cifar10_read import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 50000\n",
    "TEST_SIZE = 10000\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer1: 32x32x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n",
      "(?, 10)\n"
     ]
    }
   ],
   "source": [
    "# Graph input:\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 3], name='InputData')\n",
    "# Graph input:\n",
    "x_preprocessed = tf.placeholder(tf.float32, [None, 32, 32, 3], name='InputData_preprocessed')\n",
    "# 0-9 digits recognition => 10 classes\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "# training ? testing\n",
    "status = tf.placeholder(tf.bool, [], name='status')\n",
    "print(x.shape)\n",
    "print(y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "weight_stddev = tf.placeholder(tf.float32, [], name='weight_stddev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions to initialize varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shortcut for b(bias)\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "# shortcut for convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# shortcut for convolution layer\n",
    "def convlayer(input_layer,name,shape,status):\n",
    "    W_conv_layer = tf.get_variable('W_' + name, shape=shape,initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_conv_layer = bias_variable([shape[-1]])\n",
    "    conv_layer = conv2d(input_layer, W_conv_layer)+b_conv_layer\n",
    "    conv_layer = tf.layers.batch_normalization( inputs=conv_layer, axis=-1, momentum=0.9, epsilon=0.001, \n",
    "                                               center=True, scale=True, training = status, name = name + '_bn')\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    print(conv_layer.shape)\n",
    "    return conv_layer\n",
    "# shortcut for max_pooling layer\n",
    "def max_pool(conv):\n",
    "    return tf.nn.max_pool(conv,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "# shortcut for fully_connected layer\n",
    "def fclayer(input_layer,name,shape,keep_prob):\n",
    "    W_fclayer = tf.get_variable(\"W_\"+name, shape=shape,initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_fclayer = bias_variable([shape[-1]])\n",
    "    fclayer = tf.nn.relu(tf.matmul(input_layer, W_fclayer)+b_fclayer)\n",
    "    fclayer = tf.nn.dropout(fclayer, keep_prob)\n",
    "    print(fclayer.shape)\n",
    "    return fclayer\n",
    "# shortcut for randomnize training set\n",
    "def shuffle(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "# shortcut for dummy label\n",
    "def labelconvert(label,num_of_type=10):\n",
    "    label = label.reshape([-1,1])\n",
    "    label=np.tile(label,(1,num_of_type))\n",
    "    label_true = np.tile(np.arange(num_of_type),(label.shape[0],1))\n",
    "    return np.equal(label,label_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing ops for testing\n",
    "preprocessing = tf.map_fn(lambda img: tf.image.per_image_standardization(img), x)\n",
    "#preprocessing ops for training\n",
    "distorted_image = tf.map_fn(lambda img: tf.image.pad_to_bounding_box(img, 4, 4, 40, 40), x)\n",
    "distorted_image = tf.map_fn(lambda img: tf.random_crop(img, [32, 32, 3]), distorted_image)\n",
    "distorted_image = tf.map_fn(lambda img: tf.image.random_flip_left_right(img), distorted_image)\n",
    "distorted_image = tf.map_fn(lambda img: tf.image.random_brightness(img, max_delta=63), distorted_image)\n",
    "distorted_image = tf.map_fn(lambda img: tf.image.random_contrast(img, lower=0.2, upper=1.8), distorted_image)\n",
    "aug_preprocessing = tf.map_fn(lambda img: tf.image.per_image_standardization(img), distorted_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for convolution layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "conv1 = convlayer(x_preprocessed,'conv1',[3,3,3,64],status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for convolution layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "conv2 = convlayer(conv1,'conv2',[3,3,64,64],status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for convolution layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "conv2p = convlayer(conv2,'conv2p',[3,3,64,64],status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for max-pooling layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(16), Dimension(16), Dimension(64)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool1 = max_pool(conv2p)\n",
    "pool1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for convolution layer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 96)\n"
     ]
    }
   ],
   "source": [
    "conv3 = convlayer(pool1,'conv3',[3,3,64,96],status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for convolution layer 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 96)\n"
     ]
    }
   ],
   "source": [
    "conv4 = convlayer(conv3,'conv4',[3,3,96,96],status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for max-pooling layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(8), Dimension(8), Dimension(96)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2 = max_pool(conv4)\n",
    "pool2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for convolution layer 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8, 8, 128)\n"
     ]
    }
   ],
   "source": [
    "conv5 = convlayer(pool2,'conv5',[3,3,96,128],status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for convolution layer 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8, 8, 128)\n"
     ]
    }
   ],
   "source": [
    "conv6 = convlayer(conv5,'conv6',[3,3,128,128],status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for convolution layer 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8, 8, 128)\n"
     ]
    }
   ],
   "source": [
    "conv7 = convlayer(conv6,'conv7',[3,3,128,128],status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for convolution layer 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8, 8, 128)\n"
     ]
    }
   ],
   "source": [
    "conv8 = convlayer(conv7,'conv8',[3,3,128,128],status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for max-pooling layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(4), Dimension(4), Dimension(128)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool3 = max_pool(conv8)\n",
    "pool3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for convolution layer 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4, 4, 256)\n"
     ]
    }
   ],
   "source": [
    "conv9 = convlayer(pool3,'conv9',[3,3,128,256],status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for convolution layer 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4, 4, 256)\n"
     ]
    }
   ],
   "source": [
    "conv10 = convlayer(conv9,'conv10',[3,3,256,256],status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for max-pooling layer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(2), Dimension(2), Dimension(256)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool4 = max_pool(conv10)\n",
    "pool4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(1024)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc = tf.reshape(pool4,[-1, pool4.shape[1].value*pool4.shape[2].value*pool4.shape[3].value])\n",
    "fc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for fully connected layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 256)\n"
     ]
    }
   ],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "fc1 = fclayer(fc,'fc1',[1024,256],keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for fully connected layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128)\n"
     ]
    }
   ],
   "source": [
    "fc2 = fclayer(fc1,'fc2',[256,128],keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for fully connected layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 64)\n"
     ]
    }
   ],
   "source": [
    "fc3 = fclayer(fc2,'fc3',[128,64],keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops for output before softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(10)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_softmax = tf.get_variable(\"W_softmax\", shape=[64,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b_softmax = bias_variable([10])\n",
    "y = tf.matmul(fc3, W_softmax)+b_softmax\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#update parameter for BN\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extracting data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CIFAR10 = file_information('CIFAR10_data/binary', height=32, width=32, \n",
    "                           channels=3, labels=1, train_size=TRAIN_SIZE, test_size = TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CIFAR10_data = read_cifar10(CIFAR10)\n",
    "\n",
    "training_data,testing_data = CIFAR10_data.training_data,CIFAR10_data.testing_data\n",
    "\n",
    "#convert 1-line label to 10 dimension\n",
    "training_label = labelconvert(CIFAR10_data.training_label).astype(float)\n",
    "testing_label = labelconvert(CIFAR10_data.testing_label).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CNNtraining(LEARNING_RATE=0.001,training_data = training_data,training_label = training_label,\n",
    "                testing_data = testing_data,testing_label = testing_label,\n",
    "                kb = 0.5):\n",
    "    with tf.Session() as sess:\n",
    "        #init\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        #preprocessing training/testing data\n",
    "        training_data_augmented = training_data\n",
    "        training_label_augmented = training_label\n",
    "        training_data = preprocessing.eval(feed_dict={x: training_data})\n",
    "        testing_data = preprocessing.eval(feed_dict={x: testing_data})\n",
    "        #save for plot\n",
    "        loss_train=[]\n",
    "        accuracy_train=[]\n",
    "        accuracy_test=[]\n",
    "        for current_iter in range(2000):\n",
    "            if(len(loss_train)>30):\n",
    "                if(np.max(accuracy_train)-np.max(accuracy_train[:len(loss_train)-30])<0.001):\n",
    "                    break\n",
    "            #shuffle input data  \n",
    "            training_data_augmented,training_label_augmented = shuffle(training_data_augmented,training_label_augmented)\n",
    "            #mini-batch gradient descent\n",
    "            for i in range(int(TRAIN_SIZE/BATCH_SIZE)):\n",
    "                #augmenting training label\n",
    "                training_data_batch = aug_preprocessing.eval(feed_dict={x: training_data_augmented[BATCH_SIZE*i:BATCH_SIZE*(i+1)]})\n",
    "                training_label_batch = training_label_augmented[BATCH_SIZE*i:BATCH_SIZE*(i+1)]\n",
    "                sess.run([train_step,extra_update_ops],feed_dict={x_preprocessed: training_data_batch,\n",
    "                                          y_: training_label_batch,\n",
    "                                         learning_rate: LEARNING_RATE,\n",
    "                                         keep_prob:kb,\n",
    "                                         status:True})\n",
    "            #iteration in order to avoid OOM\n",
    "            train_accuracy = 0\n",
    "            train_loss = 0\n",
    "            test_accuracy = 0\n",
    "            for i in range(100):\n",
    "                train_accuracy += accuracy.eval(feed_dict={x_preprocessed:training_data[500*i:500*(i+1)], \n",
    "                                                           y_: training_label[500*i:500*(i+1)],\n",
    "                                                           keep_prob:1.0,\n",
    "                                                           status:False})\n",
    "                train_loss += cross_entropy.eval(feed_dict={x_preprocessed:training_data[500*i:500*(i+1)], \n",
    "                                                            y_: training_label[500*i:500*(i+1)],\n",
    "                                                            keep_prob:1.0,\n",
    "                                                            status:False})\n",
    "            train_accuracy /= 100\n",
    "            train_loss /= 100\n",
    "            for i in range(20):\n",
    "                test_accuracy += accuracy.eval(feed_dict={x_preprocessed:testing_data[500*i:500*(i+1)], \n",
    "                                                         y_: testing_label[500*i:500*(i+1)],\n",
    "                                                         keep_prob:1.0,\n",
    "                                                         status:False})\n",
    "            test_accuracy /= 20\n",
    "            print(time.asctime( time.localtime(time.time())),\":epoch %5d\"%(current_iter+1),\"training accuracy %.4g\"%train_accuracy,\"test accuracy %.5g\"%test_accuracy)\n",
    "            loss_train = np.append(loss_train, train_loss)\n",
    "            accuracy_train = np.append(accuracy_train, train_accuracy)\n",
    "            accuracy_test = np.append(accuracy_test, test_accuracy)\n",
    "        return loss_train,accuracy_train,accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def CNNdrawing(loss_train,accuracy_train,accuracy_test):\n",
    "    horizontal_axis = np.arange(len(loss_train))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(horizontal_axis, loss_train, 'r-o',label='learning loss->%.4g'%np.min(loss_train))\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(horizontal_axis, accuracy_train, 'g--',label='training accuracy->%.4g'%np.max(accuracy_train))\n",
    "    plt.plot(horizontal_axis, accuracy_test, 'b-+',label='testing accuracy->%.4g'%np.max(accuracy_test))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 18 12:27:27 2017 :epoch     1 training accuracy 0.407 test accuracy 0.4009\n",
      "Tue Apr 18 12:30:32 2017 :epoch     2 training accuracy 0.6374 test accuracy 0.6324\n",
      "Tue Apr 18 12:33:41 2017 :epoch     3 training accuracy 0.7015 test accuracy 0.6927\n",
      "Tue Apr 18 12:36:50 2017 :epoch     4 training accuracy 0.7397 test accuracy 0.7255\n",
      "Tue Apr 18 12:39:58 2017 :epoch     5 training accuracy 0.7934 test accuracy 0.7724\n",
      "Tue Apr 18 12:43:07 2017 :epoch     6 training accuracy 0.7554 test accuracy 0.735\n",
      "Tue Apr 18 12:46:16 2017 :epoch     7 training accuracy 0.804 test accuracy 0.7868\n",
      "Tue Apr 18 12:49:25 2017 :epoch     8 training accuracy 0.8281 test accuracy 0.8033\n",
      "Tue Apr 18 12:52:34 2017 :epoch     9 training accuracy 0.8255 test accuracy 0.7984\n",
      "Tue Apr 18 12:55:43 2017 :epoch    10 training accuracy 0.8425 test accuracy 0.816\n",
      "Tue Apr 18 12:58:53 2017 :epoch    11 training accuracy 0.8508 test accuracy 0.8139\n",
      "Tue Apr 18 13:02:02 2017 :epoch    12 training accuracy 0.8572 test accuracy 0.8223\n",
      "Tue Apr 18 13:05:11 2017 :epoch    13 training accuracy 0.8769 test accuracy 0.845\n",
      "Tue Apr 18 13:08:22 2017 :epoch    14 training accuracy 0.889 test accuracy 0.8506\n",
      "Tue Apr 18 13:11:32 2017 :epoch    15 training accuracy 0.8964 test accuracy 0.8579\n",
      "Tue Apr 18 13:14:42 2017 :epoch    16 training accuracy 0.8798 test accuracy 0.8383\n",
      "Tue Apr 18 13:17:52 2017 :epoch    17 training accuracy 0.8905 test accuracy 0.8519\n",
      "Tue Apr 18 13:21:02 2017 :epoch    18 training accuracy 0.8628 test accuracy 0.8238\n",
      "Tue Apr 18 13:24:12 2017 :epoch    19 training accuracy 0.9024 test accuracy 0.8589\n",
      "Tue Apr 18 13:27:22 2017 :epoch    20 training accuracy 0.8942 test accuracy 0.8509\n",
      "Tue Apr 18 13:30:32 2017 :epoch    21 training accuracy 0.9222 test accuracy 0.8679\n",
      "Tue Apr 18 13:33:42 2017 :epoch    22 training accuracy 0.8988 test accuracy 0.8541\n",
      "Tue Apr 18 13:36:51 2017 :epoch    23 training accuracy 0.9219 test accuracy 0.8726\n",
      "Tue Apr 18 13:40:01 2017 :epoch    24 training accuracy 0.9284 test accuracy 0.8772\n",
      "Tue Apr 18 13:43:11 2017 :epoch    25 training accuracy 0.9291 test accuracy 0.8721\n",
      "Tue Apr 18 13:46:22 2017 :epoch    26 training accuracy 0.9206 test accuracy 0.8692\n",
      "Tue Apr 18 13:49:32 2017 :epoch    27 training accuracy 0.9259 test accuracy 0.8708\n",
      "Tue Apr 18 13:52:42 2017 :epoch    28 training accuracy 0.9326 test accuracy 0.8765\n",
      "Tue Apr 18 13:55:53 2017 :epoch    29 training accuracy 0.9467 test accuracy 0.8887\n",
      "Tue Apr 18 13:59:03 2017 :epoch    30 training accuracy 0.9474 test accuracy 0.8865\n",
      "Tue Apr 18 14:02:12 2017 :epoch    31 training accuracy 0.9487 test accuracy 0.8856\n",
      "Tue Apr 18 14:05:23 2017 :epoch    32 training accuracy 0.9399 test accuracy 0.8775\n",
      "Tue Apr 18 14:08:32 2017 :epoch    33 training accuracy 0.9503 test accuracy 0.891\n",
      "Tue Apr 18 14:11:42 2017 :epoch    34 training accuracy 0.9502 test accuracy 0.8846\n",
      "Tue Apr 18 14:14:52 2017 :epoch    35 training accuracy 0.9534 test accuracy 0.8887\n",
      "Tue Apr 18 14:18:03 2017 :epoch    36 training accuracy 0.9563 test accuracy 0.8873\n",
      "Tue Apr 18 14:21:13 2017 :epoch    37 training accuracy 0.9579 test accuracy 0.8922\n",
      "Tue Apr 18 14:24:23 2017 :epoch    38 training accuracy 0.9571 test accuracy 0.8923\n",
      "Tue Apr 18 14:27:34 2017 :epoch    39 training accuracy 0.9539 test accuracy 0.8894\n",
      "Tue Apr 18 14:30:45 2017 :epoch    40 training accuracy 0.9561 test accuracy 0.8854\n",
      "Tue Apr 18 14:33:55 2017 :epoch    41 training accuracy 0.9552 test accuracy 0.8873\n",
      "Tue Apr 18 14:37:04 2017 :epoch    42 training accuracy 0.9384 test accuracy 0.8712\n",
      "Tue Apr 18 14:40:13 2017 :epoch    43 training accuracy 0.9636 test accuracy 0.8967\n",
      "Tue Apr 18 14:43:23 2017 :epoch    44 training accuracy 0.9653 test accuracy 0.8957\n",
      "Tue Apr 18 14:46:33 2017 :epoch    45 training accuracy 0.9659 test accuracy 0.8918\n",
      "Tue Apr 18 14:49:44 2017 :epoch    46 training accuracy 0.9618 test accuracy 0.8897\n",
      "Tue Apr 18 14:52:53 2017 :epoch    47 training accuracy 0.9618 test accuracy 0.8879\n",
      "Tue Apr 18 14:56:04 2017 :epoch    48 training accuracy 0.9573 test accuracy 0.8893\n",
      "Tue Apr 18 14:59:14 2017 :epoch    49 training accuracy 0.9689 test accuracy 0.8927\n",
      "Tue Apr 18 15:02:24 2017 :epoch    50 training accuracy 0.965 test accuracy 0.8907\n",
      "Tue Apr 18 15:05:33 2017 :epoch    51 training accuracy 0.9634 test accuracy 0.8919\n",
      "Tue Apr 18 15:08:43 2017 :epoch    52 training accuracy 0.9725 test accuracy 0.8932\n",
      "Tue Apr 18 15:11:52 2017 :epoch    53 training accuracy 0.9673 test accuracy 0.8874\n",
      "Tue Apr 18 15:15:01 2017 :epoch    54 training accuracy 0.9712 test accuracy 0.8936\n",
      "Tue Apr 18 15:18:11 2017 :epoch    55 training accuracy 0.9717 test accuracy 0.8966\n",
      "Tue Apr 18 15:21:21 2017 :epoch    56 training accuracy 0.9724 test accuracy 0.8972\n",
      "Tue Apr 18 15:24:31 2017 :epoch    57 training accuracy 0.9685 test accuracy 0.8965\n",
      "Tue Apr 18 15:27:41 2017 :epoch    58 training accuracy 0.9754 test accuracy 0.9006\n",
      "Tue Apr 18 15:30:51 2017 :epoch    59 training accuracy 0.9663 test accuracy 0.8923\n",
      "Tue Apr 18 15:34:01 2017 :epoch    60 training accuracy 0.9765 test accuracy 0.8947\n",
      "Tue Apr 18 15:37:11 2017 :epoch    61 training accuracy 0.9764 test accuracy 0.899\n",
      "Tue Apr 18 15:40:21 2017 :epoch    62 training accuracy 0.9759 test accuracy 0.894\n",
      "Tue Apr 18 15:43:31 2017 :epoch    63 training accuracy 0.9792 test accuracy 0.9011\n",
      "Tue Apr 18 15:46:41 2017 :epoch    64 training accuracy 0.9764 test accuracy 0.9003\n",
      "Tue Apr 18 15:49:51 2017 :epoch    65 training accuracy 0.9789 test accuracy 0.8962\n",
      "Tue Apr 18 15:53:01 2017 :epoch    66 training accuracy 0.9684 test accuracy 0.8857\n",
      "Tue Apr 18 15:56:11 2017 :epoch    67 training accuracy 0.9772 test accuracy 0.8944\n",
      "Tue Apr 18 15:59:22 2017 :epoch    68 training accuracy 0.9734 test accuracy 0.89\n",
      "Tue Apr 18 16:02:32 2017 :epoch    69 training accuracy 0.9712 test accuracy 0.8888\n",
      "Tue Apr 18 16:05:43 2017 :epoch    70 training accuracy 0.9786 test accuracy 0.8958\n",
      "Tue Apr 18 16:08:54 2017 :epoch    71 training accuracy 0.9814 test accuracy 0.9005\n",
      "Tue Apr 18 16:12:04 2017 :epoch    72 training accuracy 0.9795 test accuracy 0.8995\n",
      "Tue Apr 18 16:15:15 2017 :epoch    73 training accuracy 0.9788 test accuracy 0.8985\n",
      "Tue Apr 18 16:18:26 2017 :epoch    74 training accuracy 0.9792 test accuracy 0.8983\n",
      "Tue Apr 18 16:21:37 2017 :epoch    75 training accuracy 0.9832 test accuracy 0.9035\n",
      "Tue Apr 18 16:24:48 2017 :epoch    76 training accuracy 0.9775 test accuracy 0.8945\n",
      "Tue Apr 18 16:27:59 2017 :epoch    77 training accuracy 0.9728 test accuracy 0.8973\n",
      "Tue Apr 18 16:31:09 2017 :epoch    78 training accuracy 0.9792 test accuracy 0.8984\n",
      "Tue Apr 18 16:34:20 2017 :epoch    79 training accuracy 0.9789 test accuracy 0.8957\n",
      "Tue Apr 18 16:37:30 2017 :epoch    80 training accuracy 0.9833 test accuracy 0.905\n",
      "Tue Apr 18 16:40:42 2017 :epoch    81 training accuracy 0.9851 test accuracy 0.9026\n",
      "Tue Apr 18 16:43:52 2017 :epoch    82 training accuracy 0.9749 test accuracy 0.8913\n",
      "Tue Apr 18 16:47:02 2017 :epoch    83 training accuracy 0.9823 test accuracy 0.8991\n",
      "Tue Apr 18 16:50:13 2017 :epoch    84 training accuracy 0.9834 test accuracy 0.8997\n",
      "Tue Apr 18 16:53:23 2017 :epoch    85 training accuracy 0.9796 test accuracy 0.8975\n",
      "Tue Apr 18 16:56:33 2017 :epoch    86 training accuracy 0.9856 test accuracy 0.9037\n",
      "Tue Apr 18 16:59:44 2017 :epoch    87 training accuracy 0.9838 test accuracy 0.9056\n",
      "Tue Apr 18 17:02:55 2017 :epoch    88 training accuracy 0.9861 test accuracy 0.9011\n",
      "Tue Apr 18 17:06:05 2017 :epoch    89 training accuracy 0.9835 test accuracy 0.8987\n",
      "Tue Apr 18 17:09:16 2017 :epoch    90 training accuracy 0.986 test accuracy 0.9025\n",
      "Tue Apr 18 17:12:27 2017 :epoch    91 training accuracy 0.9865 test accuracy 0.9064\n",
      "Tue Apr 18 17:15:37 2017 :epoch    92 training accuracy 0.9827 test accuracy 0.8989\n",
      "Tue Apr 18 17:18:46 2017 :epoch    93 training accuracy 0.9834 test accuracy 0.8987\n",
      "Tue Apr 18 17:21:56 2017 :epoch    94 training accuracy 0.9809 test accuracy 0.8951\n",
      "Tue Apr 18 17:25:06 2017 :epoch    95 training accuracy 0.9884 test accuracy 0.9031\n",
      "Tue Apr 18 17:28:16 2017 :epoch    96 training accuracy 0.9708 test accuracy 0.8876\n",
      "Tue Apr 18 17:31:26 2017 :epoch    97 training accuracy 0.9851 test accuracy 0.9041\n",
      "Tue Apr 18 17:34:37 2017 :epoch    98 training accuracy 0.9858 test accuracy 0.905\n",
      "Tue Apr 18 17:37:47 2017 :epoch    99 training accuracy 0.9795 test accuracy 0.896\n",
      "Tue Apr 18 17:40:58 2017 :epoch   100 training accuracy 0.985 test accuracy 0.9057\n",
      "Tue Apr 18 17:44:09 2017 :epoch   101 training accuracy 0.9879 test accuracy 0.9066\n",
      "Tue Apr 18 17:47:18 2017 :epoch   102 training accuracy 0.9857 test accuracy 0.9061\n",
      "Tue Apr 18 17:50:29 2017 :epoch   103 training accuracy 0.9861 test accuracy 0.8999\n",
      "Tue Apr 18 17:53:43 2017 :epoch   104 training accuracy 0.9824 test accuracy 0.8936\n",
      "Tue Apr 18 17:56:55 2017 :epoch   105 training accuracy 0.9851 test accuracy 0.9039\n",
      "Tue Apr 18 18:00:04 2017 :epoch   106 training accuracy 0.9851 test accuracy 0.9018\n",
      "Tue Apr 18 18:03:14 2017 :epoch   107 training accuracy 0.9852 test accuracy 0.902\n",
      "Tue Apr 18 18:06:24 2017 :epoch   108 training accuracy 0.9884 test accuracy 0.9079\n",
      "Tue Apr 18 18:09:34 2017 :epoch   109 training accuracy 0.9901 test accuracy 0.9037\n",
      "Tue Apr 18 18:12:45 2017 :epoch   110 training accuracy 0.9851 test accuracy 0.9004\n",
      "Tue Apr 18 18:15:55 2017 :epoch   111 training accuracy 0.9881 test accuracy 0.9038\n",
      "Tue Apr 18 18:19:05 2017 :epoch   112 training accuracy 0.9817 test accuracy 0.8923\n",
      "Tue Apr 18 18:22:15 2017 :epoch   113 training accuracy 0.9866 test accuracy 0.9035\n",
      "Tue Apr 18 18:25:25 2017 :epoch   114 training accuracy 0.9854 test accuracy 0.9039\n",
      "Tue Apr 18 18:28:35 2017 :epoch   115 training accuracy 0.9832 test accuracy 0.899\n",
      "Tue Apr 18 18:31:46 2017 :epoch   116 training accuracy 0.9872 test accuracy 0.9032\n",
      "Tue Apr 18 18:34:56 2017 :epoch   117 training accuracy 0.9841 test accuracy 0.9002\n",
      "Tue Apr 18 18:38:07 2017 :epoch   118 training accuracy 0.9885 test accuracy 0.9085\n",
      "Tue Apr 18 18:41:17 2017 :epoch   119 training accuracy 0.9866 test accuracy 0.9032\n",
      "Tue Apr 18 18:44:27 2017 :epoch   120 training accuracy 0.987 test accuracy 0.8997\n",
      "Tue Apr 18 18:47:37 2017 :epoch   121 training accuracy 0.9856 test accuracy 0.8982\n",
      "Tue Apr 18 18:50:47 2017 :epoch   122 training accuracy 0.9833 test accuracy 0.8975\n",
      "Tue Apr 18 18:53:57 2017 :epoch   123 training accuracy 0.9855 test accuracy 0.9019\n",
      "Tue Apr 18 18:57:08 2017 :epoch   124 training accuracy 0.9901 test accuracy 0.9058\n",
      "Tue Apr 18 19:00:18 2017 :epoch   125 training accuracy 0.9915 test accuracy 0.9059\n",
      "Tue Apr 18 19:03:28 2017 :epoch   126 training accuracy 0.9859 test accuracy 0.9041\n",
      "Tue Apr 18 19:06:38 2017 :epoch   127 training accuracy 0.9869 test accuracy 0.8997\n",
      "Tue Apr 18 19:09:48 2017 :epoch   128 training accuracy 0.9873 test accuracy 0.9038\n",
      "Tue Apr 18 19:12:59 2017 :epoch   129 training accuracy 0.9913 test accuracy 0.908\n",
      "Tue Apr 18 19:16:09 2017 :epoch   130 training accuracy 0.989 test accuracy 0.9054\n",
      "Tue Apr 18 19:19:20 2017 :epoch   131 training accuracy 0.9889 test accuracy 0.9044\n",
      "Tue Apr 18 19:22:30 2017 :epoch   132 training accuracy 0.9902 test accuracy 0.909\n",
      "Tue Apr 18 19:25:40 2017 :epoch   133 training accuracy 0.9903 test accuracy 0.9059\n",
      "Tue Apr 18 19:28:50 2017 :epoch   134 training accuracy 0.9881 test accuracy 0.9056\n",
      "Tue Apr 18 19:32:00 2017 :epoch   135 training accuracy 0.9892 test accuracy 0.9072\n",
      "Tue Apr 18 19:35:10 2017 :epoch   136 training accuracy 0.9922 test accuracy 0.9082\n",
      "Tue Apr 18 19:38:19 2017 :epoch   137 training accuracy 0.9896 test accuracy 0.9068\n",
      "Tue Apr 18 19:41:29 2017 :epoch   138 training accuracy 0.9862 test accuracy 0.9033\n",
      "Tue Apr 18 19:44:40 2017 :epoch   139 training accuracy 0.9901 test accuracy 0.9034\n",
      "Tue Apr 18 19:47:49 2017 :epoch   140 training accuracy 0.9899 test accuracy 0.9079\n",
      "Tue Apr 18 19:50:59 2017 :epoch   141 training accuracy 0.9899 test accuracy 0.9072\n",
      "Tue Apr 18 19:54:09 2017 :epoch   142 training accuracy 0.9914 test accuracy 0.9045\n",
      "Tue Apr 18 19:57:19 2017 :epoch   143 training accuracy 0.9912 test accuracy 0.9046\n",
      "Tue Apr 18 20:00:28 2017 :epoch   144 training accuracy 0.9851 test accuracy 0.9014\n",
      "Tue Apr 18 20:03:39 2017 :epoch   145 training accuracy 0.9896 test accuracy 0.9063\n",
      "Tue Apr 18 20:06:49 2017 :epoch   146 training accuracy 0.9912 test accuracy 0.9087\n",
      "Tue Apr 18 20:09:59 2017 :epoch   147 training accuracy 0.9906 test accuracy 0.9037\n",
      "Tue Apr 18 20:13:09 2017 :epoch   148 training accuracy 0.989 test accuracy 0.9032\n",
      "Tue Apr 18 20:16:20 2017 :epoch   149 training accuracy 0.9878 test accuracy 0.9005\n",
      "Tue Apr 18 20:19:30 2017 :epoch   150 training accuracy 0.991 test accuracy 0.9052\n",
      "Tue Apr 18 20:22:40 2017 :epoch   151 training accuracy 0.9861 test accuracy 0.9012\n",
      "Tue Apr 18 20:25:49 2017 :epoch   152 training accuracy 0.9876 test accuracy 0.8993\n",
      "Tue Apr 18 20:29:00 2017 :epoch   153 training accuracy 0.9913 test accuracy 0.9103\n",
      "Tue Apr 18 20:32:10 2017 :epoch   154 training accuracy 0.9882 test accuracy 0.9039\n",
      "Tue Apr 18 20:35:19 2017 :epoch   155 training accuracy 0.988 test accuracy 0.9005\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nx8k5fwAuimzw6IBkTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19859f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#drop prob1=drop prob2 = 1.0\n",
    "loss_train,accuracy_train,accuracy_test = CNNtraining(kb = 0.9)\n",
    "CNNdrawing(loss_train,accuracy_train,accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add layer\n",
    "\n",
    "Augmentation\n",
    "\n",
    "functionalized\n",
    "\n",
    "Batch Norm\n",
    "\n",
    "Shrinked VGG\n",
    "\n",
    "Adam optimizer(default)\n",
    "\n",
    "dropoff\n",
    "\n",
    "stop if 0.5% accracy improvement is not satisfied within 30 epoch"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
